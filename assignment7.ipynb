{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics,preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from pydotplus import graph_from_dot_data\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the Scikit-Learn Library train the Decision Tree Classifier to the attached PhishingVsBenignURL data set using all of the features at once. (Dataset is originally from here: https://www.unb.ca/cic/datasets/url-2016.html )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training phishing 1896 samples\n",
      "Total training benign 1896 samples\n",
      "Total test phishing 813 samples\n",
      "Total test benign 813 samples\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "target_index = ['benign','phishing']\n",
    "data = pd.read_csv('DataSetForPhishingVSBenignUrl.csv')\n",
    "data = data[data.URL_Type_obf_Type.isin(target_index)].dropna().reset_index(drop=True)\n",
    "rus = RandomUnderSampler(random_state=seed) # random balance sample function\n",
    "standard = preprocessing.StandardScaler() # standardize data func\n",
    "X,y = rus.fit_resample(standard.fit_transform(data[data.columns[:-1]].values),\n",
    "                       data[data.columns[-1]].values)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=seed,stratify=y)\n",
    "print(\"Total training phishing {} samples\\nTotal training benign {} samples\".format(\n",
    "                np.sum(y_train == 'phishing'),np.sum(y_train == 'benign')))\n",
    "print(\"Total test phishing {} samples\\nTotal test benign {} samples\".format(\n",
    "                np.sum(y_test == 'phishing'),np.sum(y_test == 'benign')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "gini impurity max tree depth 1 accuracy: 80.69%\n",
      "Adaboost with tree base depth 1 accuracy: 96.0%\n",
      "================================================================================\n",
      "gini impurity max tree depth 3 accuracy: 90.84%\n",
      "Adaboost with tree base depth 3 accuracy: 97.48%\n",
      "================================================================================\n",
      "gini impurity max tree depth 6 accuracy: 94.59%\n",
      "Adaboost with tree base depth 6 accuracy: 96.92%\n",
      "================================================================================\n",
      "gini impurity max tree depth 9 accuracy: 94.53%\n",
      "Adaboost with tree base depth 9 accuracy: 96.74%\n",
      "================================================================================\n",
      "gini impurity max tree depth 12 accuracy: 94.9%\n",
      "Adaboost with tree base depth 12 accuracy: 96.43%\n",
      "================================================================================\n",
      "gini impurity max tree depth 15 accuracy: 95.08%\n",
      "Adaboost with tree base depth 15 accuracy: 95.14%\n",
      "================================================================================\n",
      "gini impurity max tree depth 18 accuracy: 95.08%\n",
      "Adaboost with tree base depth 18 accuracy: 95.08%\n",
      "================================================================================\n",
      "entropy impurity max tree depth 1 accuracy: 80.69%\n",
      "Adaboost with tree base depth 1 accuracy: 95.88%\n",
      "================================================================================\n",
      "entropy impurity max tree depth 3 accuracy: 90.34%\n",
      "Adaboost with tree base depth 3 accuracy: 97.36%\n",
      "================================================================================\n",
      "entropy impurity max tree depth 6 accuracy: 93.91%\n",
      "Adaboost with tree base depth 6 accuracy: 97.05%\n",
      "================================================================================\n",
      "entropy impurity max tree depth 9 accuracy: 95.39%\n",
      "Adaboost with tree base depth 9 accuracy: 97.05%\n",
      "================================================================================\n",
      "entropy impurity max tree depth 12 accuracy: 95.76%\n",
      "Adaboost with tree base depth 12 accuracy: 96.99%\n",
      "================================================================================\n",
      "entropy impurity max tree depth 15 accuracy: 95.69%\n",
      "Adaboost with tree base depth 15 accuracy: 95.94%\n",
      "================================================================================\n",
      "entropy impurity max tree depth 18 accuracy: 96.13%\n",
      "Adaboost with tree base depth 18 accuracy: 96.06%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "tree_models = [[],[]]\n",
    "adas = [[],[]]\n",
    "accuracies = [[],[]]\n",
    "criterion = ['gini', 'entropy']\n",
    "for j,c in enumerate(criterion):\n",
    "    for i,d in enumerate([1,3,6,9,12,15,18]):\n",
    "        tree_models[j].append(DecisionTreeClassifier(criterion=c,max_depth=d,random_state=seed))\n",
    "        adas[j].append(AdaBoostClassifier(base_estimator=tree_models[j][i],\n",
    "                                          n_estimators=500,\n",
    "                                          learning_rate=0.1,random_state=seed))\n",
    "        tree_models[j][i].fit(X_train, y_train)\n",
    "        adas[j][i].fit(X_train,y_train)\n",
    "        accuracies[j].append(np.round(tree_models[j][i].score(X_test,y_test)*100,2))\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{c} impurity max tree depth {d} accuracy: {accuracies[j][i]}%\")\n",
    "        print(f\"Adaboost with tree base depth {d} accuracy: {np.round(adas[j][i].score(X_test,y_test)*100,2)}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the Scikit Learn AdaBoost Classifier code to the dataset for classifying phishing vs benign using and all feature at once and upload your .ipynb file. Use a Decision Tree Classifier at your base classifier. Use decision trees of varying depths(1,3,6,9,12,15,18 for both gini and entropy criterion) for the base classifier. Compare your results with those you obtained last week when you used the Scikit Decision Tree Classifier(Week 5 assignment)\n",
    "\n",
    "* Using Gini impurity with tree depth 1, adaboost is significantly better than only use one tree i.e. 80.69% accuracy for tree vs. 96% accuracy for adaboost. Similarly, a tree depth of 3 and 6, adaboost is also significantlt better than tree itself i.e. 90.84% accuracy for tree only vs. 97.48% accuracy for adaboost. For depth of 6, 9 and 12 have very similar result because there is no significantly difference. However, using a larger size of tree depth, the accuracy of adaboost is decreasing. On the other hand, tree only accuracy is more closer to adaboost. In the case of using tree depth 15 and 18, they have same result.\n",
    "* Using Entropy impurity with tree depth 1 and 3, adaboost is significantly better than only use one tree i.e. 80.69% accuracy for tree vs. 95.88% accuracy for adaboost and 90.34% accuracy for tree vs. 97.36% accuracy for adaboost respectively. However, for depth of 6, 9, 15 and 18 have very similar or no significantly different result. Especially when the depths are 15 and 18. When the depth size is larger, it seems like entropy is better than gini but no significantly difference. However, when we use larger depth, i.e. greater 6 the running time of adaboost take much longer comparing to tree itself only. In conclusion, if the depth is greater than or equal to 6 there is no need to use Adaboost at all due to the computation complexity and they have very similar result. On the other hand, Adaboost has significantly improvement for the case of tree depth less than 6 with highest accuracy i.e. the case of depth 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
