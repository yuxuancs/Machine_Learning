{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics,preprocessing,svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from pydotplus import graph_from_dot_data\n",
    "from IPython.display import Image\n",
    "from keras import layers, models\n",
    "from keras.optimizer_v2 import adam\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.utils import vis_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset citation:\n",
    "https://research.aalto.fi/en/datasets/phishstorm-phishing-legitimate-url-dataset\n",
    "\"\n",
    "Description\n",
    "URLs dataset with features built and used for evaluation in the paper \"PhishStorm: Detecting Phishing with Streaming Analytics\" published in IEEE TNSM.\n",
    "The dataset contains 96,018 URLs: 48,009 legitimate URLs and 48,009 phishing URLs.\n",
    "\n",
    "This is a CSV file where the \"domain\" column provides a unique identifier for each entry (which is actually a URL). The \"label\" column provides the domain entry status, 0: legitimate / 1:phishing.\n",
    "Other columns provide computed values for features introduced in [1].\n",
    "\n",
    "Please refer to the following publication when citing this dataset:\n",
    "[1] S. Marchal, J. Francois, R. State, and T. Engel. PhishStorm: Detecting Phishing with Streaming Analytics. IEEE Transactions on Network and Service Management (TNSM), 11(4):458-471, 2014.\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 69 # fix random seed\n",
    "df = pd.read_csv('urlset.csv',\n",
    "                 encoding='UTF-8',\n",
    "                 encoding_errors='ignore',\n",
    "                 on_bad_lines='skip',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### there are some unreadable row, we have to skip therefore the dataset contains total 96003 samples. However, there is two duplicates values, we have to drop them. Therefore, the dataset contains total 96001 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove other unneccessary columns, only keep url and label and make sure url type is string\n",
    "df = df[['domain','label']].astype({'domain':str,'label':float})\n",
    "df = df.drop_duplicates(ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_char = list(''.join(df.domain.values.tolist()))\n",
    "unique_char = np.unique(all_char,return_counts=True)\n",
    "print(\"Raw data total {} unique characters:\".format(unique_char[0].shape[0]))\n",
    "print(unique_char[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### preprocessing data\n",
    "1. fit character into numerical values\n",
    "2. only keep the url length up to 75 characters\n",
    "3. if url does not have 75 characters then padding in front with 0\n",
    "4. remove all $\\pm\\infty$ and nan values\n",
    "5. balance binary dataset\n",
    "-------------------------------------\n",
    "6. use PCA to reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tok = text.Tokenizer(char_level=True) # character function\n",
    "char_tok.fit_on_texts(df.domain) # fit character\n",
    "data = char_tok.texts_to_sequences(df.domain)\n",
    "num_char = len(char_tok.word_index)+1\n",
    "max_length = max([len(n) for n in data])\n",
    "length = 75\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "print(\"Total {} possible length of characters\".format(max_length)) # check maximum possible length\n",
    "print('='*80)\n",
    "data = sequence.pad_sequences(data,maxlen=length,dtype=np.float64) \n",
    "print(\"Total {} samples and only use {} features\".format(*data.shape))\n",
    "print('='*80)\n",
    "# save data and its target to a new dataframe\n",
    "data = pd.DataFrame(np.hstack((data,df[['label']].values)))\n",
    "# drop nan, inf, or -inf values\n",
    "data = data.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "# use PCA to reduce dimension of feature\n",
    "columns = data.columns\n",
    "rus = RandomUnderSampler(random_state=seed) # random balance smaple function\n",
    "standard = preprocessing.StandardScaler() # standardize data function\n",
    "X, y = rus.fit_resample(data[columns[:-1]],data[columns[-1]])\n",
    "print(\"After balance sampling:\")\n",
    "print(f\"|phishing:  {len(y[y==1])}  | non-phishing: {len(y[y==0])}\")\n",
    "print('='*80)\n",
    "# split dataset for training, validation and testing\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=seed,stratify=y)\n",
    "print(\"split dataset into training and testing:\")\n",
    "print(f\"Training size: {y_train.shape[0]}; Testing size: {y_test.shape[0]}\")\n",
    "print('='*80)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(num_char=373,length=75):\n",
    "    '''\n",
    "    convolutional neural network\n",
    "    '''\n",
    "    model = models.Sequential([Embedding(num_char,128,input_length=length), # embedding\n",
    "                               layers.Dropout(0.5), \n",
    "                               layers.Conv1D(256, 3), # conv layer 1\n",
    "                               layers.ELU(), # Exponential Linear Unit\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Conv1D(512, 6), # conv layer 2\n",
    "                               layers.ELU(), # Exponential Linear Unit\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Conv1D(1024,8), # conv layer 3\n",
    "                               layers.ELU(), # Exponential Linear Unit\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Flatten(),\n",
    "                               layers.Dense(1024,activation='relu'), # hidden layer 1\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Dense(256, activation='relu'), # hidden layer 2\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Dense(64,  activation='relu'), # hidden layer 3\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Dense(1,activation='sigmoid')]) # output layer\n",
    "    model_compile(model)\n",
    "    return model\n",
    "\n",
    "def LSTM_model(num_char=373,length=75):\n",
    "    '''\n",
    "    model with one embedding layer,\n",
    "    follow up with 25% drop out and batach normalization\n",
    "    lstm layer with 128 units and 50% recurrent drop out\n",
    "    '''\n",
    "    model =models.Sequential([Embedding(num_char,128,input_length=length), # embedding\n",
    "                              layers.Dropout(0.25),\n",
    "                              layers.BatchNormalization(), # batach normalization \n",
    "                              layers.LSTM(128,recurrent_dropout=0.5), # long-short term memory\n",
    "                              layers.Dropout(0.5),\n",
    "                              layers.Flatten(),\n",
    "                              layers.Dense(1,activation='sigmoid')]) # output layer\n",
    "    model_compile(model)\n",
    "    return model\n",
    "\n",
    "def LSTM_larger_model(num_char=373,length=75):\n",
    "    '''\n",
    "    model with one embedding layer,\n",
    "    follow up with 25% drop out and batach normalization\n",
    "    lstm layer with 256 units and 50% recurrent drop out\n",
    "    lstm layer with 512 units and 50% recurrent drop out\n",
    "    '''\n",
    "    model =models.Sequential([Embedding(num_char,128,input_length=length), # embedding\n",
    "                              layers.Dropout(0.25),\n",
    "                              layers.BatchNormalization(), # batach normalization \n",
    "                              layers.LSTM(256,recurrent_dropout=0.5,\n",
    "                                          return_sequences=True,\n",
    "                                          input_shape=(75,128)), # long-short term memory\n",
    "                              layers.Dropout(0.5),\n",
    "                              layers.BatchNormalization(), # batach normalization\n",
    "                              layers.LSTM(512,recurrent_dropout=0.5), # long-short term memory\n",
    "                              layers.Dropout(0.5),\n",
    "                              layers.Flatten(),\n",
    "                              layers.Dense(1,activation='sigmoid')]) # output layer\n",
    "    model_compile(model)\n",
    "    return model\n",
    "    \n",
    "\n",
    "def model_compile(model):\n",
    "    '''\n",
    "    model compile with binary corss entropy, adam optimizer with learning rate 0.005\n",
    "    '''\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam.Adam(learning_rate=0.005),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "def train_model(model,X_train,y_train,epochs=10,batch_size=32,filename='cnn'):\n",
    "#     print(\"model summary\")\n",
    "#     print(model.summary())\n",
    "    print(f\"training model with {epochs} epochs and batches of size {batch_size}:\")\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint(filename+'.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7,\n",
    "                                       verbose=1,min_delta=1e-4, mode='min')\n",
    "    return model.fit(X_train,y_train,validation_split=0.2,epochs=epochs,\n",
    "                     batch_size=batch_size,use_multiprocessing=True,\n",
    "                     callbacks=[earlyStopping, mcp_save,reduce_lr_loss])\n",
    "\n",
    "def load_best_model(model,filename='cnn'):\n",
    "    return model.load_weights(filepath = filename+'.hdf5')\n",
    "\n",
    "\n",
    "def evaluate_model(model,X_test,y_test):\n",
    "    print(\"model summary\")\n",
    "    print(model.summary())\n",
    "    print(f\"testing model:\")\n",
    "    result = model.evaluate(X_test,y_test)\n",
    "    print(\"Model score: {:.04f} and accuracy: {:0.2f}%\".format(result[0],result[1]*100))\n",
    "    \n",
    "def classification_report(model,X_test,y_test):\n",
    "    print(\"claissification report:\")\n",
    "    print(metrics.classification_report(y_test,\n",
    "                                  np.where(model.predict(X_test,\n",
    "                                                         use_multiprocessing=True)>0.5,\n",
    "                                           1,0)))\n",
    "    \n",
    "def export_plot(model,fname='cnn'):\n",
    "    '''\n",
    "    plot modell's structure\n",
    "    '''\n",
    "    vis_utils.plot_model(model,to_file=fname+'.png',show_shapes=True,\n",
    "                         show_layer_names=True,expand_nested=True,dpi=96,)\n",
    "    \n",
    "def plot_acc(ax,hist,title):\n",
    "    val = title if title == 'Accuracy' else 'Loss'\n",
    "    ax.plot(hist.history[val.lower()],lw=2)\n",
    "    ax.plot(hist.history['val_'+val.lower()],lw=2)\n",
    "    ax.set(title=title,ylabel=val,xlabel='Epoch')\n",
    "    ax.legend(['Train','Test'],fontsize='xx-large')\n",
    "    ax.grid(True)\n",
    "\n",
    "def false_negative_rate(cm):\n",
    "    '''\n",
    "    compute false negative rate\n",
    "    '''\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return fn/(fn+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN_model(num_char,length)\n",
    "export_plot(cnn_model,'cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_hist = train_model(cnn_model,X_train,y_train,filename='cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_best_model(cnn_model,filename='cnn')\n",
    "plt.rcParams.update({'font.size':15})\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,8))\n",
    "ax = ax.flatten()\n",
    "plot_acc(ax[0],cnn_hist,'Accuracy')\n",
    "plot_acc(ax[1],cnn_hist,'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(cnn_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "y_pred = np.where(cnn_model.predict(X_test,use_multiprocessing=True)>0.5,1,0)\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:'\n",
    "          '{:0.2f}%'.format(metrics.accuracy_score(y_test,y_pred)*100),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(cnn_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_model(num_char,length)\n",
    "export_plot(lstm_model,'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lstm_hist = train_model(lstm_model,X_train,y_train,filename='lstm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_best_model(lstm_model,filename='lstm1')\n",
    "plt.rcParams.update({'font.size':15})\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,8))\n",
    "ax = ax.flatten()\n",
    "plot_acc(ax[0],lstm_hist,'Accuracy')\n",
    "plot_acc(ax[1],lstm_hist,'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lstm_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "y_pred = np.where(lstm_model.predict(X_test,use_multiprocessing=True)>0.5,1,0)\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:'\n",
    "          '{:0.2f}%'.format(metrics.accuracy_score(y_test,y_pred)*100),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(lstm_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_lstm = LSTM_larger_model(num_char,length)\n",
    "export_plot(larger_lstm,'larger_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_lstm_hist = train_model(larger_lstm,X_train,y_train,filename='lstm2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_best_model(larger_lstm,filename='lstm2')\n",
    "plt.rcParams.update({'font.size':15})\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,8))\n",
    "ax = ax.flatten()\n",
    "plot_acc(ax[0],larger_lstm_hist,'Accuracy')\n",
    "plot_acc(ax[1],larger_lstm_hist,'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(larger_lstm,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "y_pred = np.where(larger_lstm.predict(X_test,use_multiprocessing=True)>0.5,1,0)\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:'\n",
    "          '{:0.2f}%'.format(metrics.accuracy_score(y_test,y_pred)*100),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classification_report(larger_lstm,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression without feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression without feature reduction:\")\n",
    "logit = LogisticRegression(penalty='l2',tol=0.0001,C=1,random_state=seed,\n",
    "                           solver='liblinear',multi_class='ovr',max_iter=100,verbose=0)\n",
    "logit.fit(X_train,y_train)\n",
    "test_score = np.round(logit.score(X_test,y_test)*100,2)\n",
    "print('='*80)\n",
    "print(f\"|+| test accuracy: \\t{test_score}%\")\n",
    "print('='*80)\n",
    "cm = metrics.confusion_matrix(y_test,logit.predict(X_test))\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:'\n",
    "          '{:0.2f}%'.format(test_score),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modified from: Selecting dimensionality reduction with Pipeline and GridSearchCV\n",
    "orginal authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre\n",
    "'''\n",
    "pipe = Pipeline([# the reduce_dim stage is populated by the param_grid\n",
    "                 (\"reduce_dim\", \"passthrough\"),\n",
    "                 (\"classify\", svm.LinearSVC(dual=False, max_iter=10000)),\n",
    "                ])\n",
    "\n",
    "N_FEATURES_OPTIONS = [15,30,45,60] # reduction features\n",
    "C_OPTIONS = [1,10]\n",
    "param_grid = [{\"reduce_dim\": [PCA(iterated_power=7,random_state=seed)],\n",
    "               \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n",
    "               \"classify__C\": C_OPTIONS,}]\n",
    "reducer_labels = [\"PCA\"]\n",
    "\n",
    "grid = GridSearchCV(pipe,n_jobs=1,param_grid=param_grid)\n",
    "grid.fit(X, y)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_[\"mean_test_score\"])\n",
    "# scores are in the order of param_grid iteration, which is alphabetical\n",
    "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "# select score for best C\n",
    "mean_scores = mean_scores.max(axis=0)\n",
    "bar_offsets = np.arange(len(N_FEATURES_OPTIONS)) * (len(reducer_labels) + 1) + 0.5\n",
    "\n",
    "plt.figure()\n",
    "COLORS = \"bgrcmyk\"\n",
    "for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n",
    "    plt.bar(bar_offsets+i,reducer_scores,label=label,color=COLORS[i])\n",
    "\n",
    "plt.title(\"Comparing feature reduction techniques\")\n",
    "plt.xlabel(\"Reduced number of features\")\n",
    "plt.xticks(bar_offsets, N_FEATURES_OPTIONS)\n",
    "plt.ylabel(\"classification accuracy\")\n",
    "plt.ylim((0, 1))\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "# only use 30 features from pca\n",
    "pca = PCA(n_components=15,iterated_power=7,random_state=seed)\n",
    "new_X = pca.fit_transform(X)\n",
    "\n",
    "# standardize data\n",
    "new_X = standard.fit_transform(new_X) # normalize all features\n",
    "std_X = new_X[X_train.index.values,:] # allocate X train, sample \n",
    "std_y = y.values[y_train.index.values] # allocate train target\n",
    "std_X_test = new_X[X_test.index.values,:] # allocate X test \n",
    "std_y_test = y.values[y_test.index.values] # allocate test target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(penalty='l2',tol=0.0001,C=1,random_state=seed,\n",
    "                           solver='liblinear',multi_class='ovr',max_iter=100,verbose=1)\n",
    "logit.fit(std_X,std_y)\n",
    "test_score = np.round(logit.score(std_X_test,std_y_test)*100,2)\n",
    "print('='*80)\n",
    "print(f\"|+| test accuracy: \\t{test_score}%\")\n",
    "print('='*80)\n",
    "cm = metrics.confusion_matrix(std_y_test,logit.predict(std_X_test))\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:{}%'.format(test_score),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = metrics.precision_recall_curve(std_y_test,logit.predict(std_X_test))\n",
    "disp = metrics.PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "disp.plot()\n",
    "disp.ax_.set(title=\"Precision-Recall curve\")\n",
    "disp.ax_.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1 # SVM regularization parameter\n",
    "max_iter = 100000\n",
    "svm_models = [svm.LinearSVC(C=C,max_iter=max_iter),\n",
    "              svm.SVC(kernel='rbf',gamma='auto',max_iter=max_iter,C=C)]\n",
    "svm_models = [clf.fit(std_X,std_y) for clf in svm_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_titles = ['Linear SVC', 'SVC with RBF kernel']\n",
    "for i, clf in enumerate(svm_models):\n",
    "    test_score = np.round(clf.score(std_X_test,std_y_test)*100,2)\n",
    "    print(\"=\"*80)\n",
    "    print(f\"||{svm_titles[i]}\\ttesting score: {test_score}\")\n",
    "    print('='*80)\n",
    "    print(f\"|+| test accuracy: \\t{test_score}%\")\n",
    "    print('='*80)\n",
    "    cm = metrics.confusion_matrix(std_y_test,clf.predict(std_X_test))\n",
    "    print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "    print('='*80)\n",
    "    plt.rcParams.update({'font.size':15})\n",
    "    ax = plt.figure(figsize=(8,8))\n",
    "    sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion matrix\\nTesting accuracy score:{}%'.format(test_score),size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_models = []\n",
    "accuracies = []\n",
    "criterion = ['gini','entropy']\n",
    "depth = 3\n",
    "for j, c in enumerate(criterion):\n",
    "    tree_models.append(DecisionTreeClassifier(criterion=c,max_depth=depth,random_state=seed))\n",
    "    tree_models[j].fit(std_X,std_y)\n",
    "    accuracies.append(np.round(tree_models[j].score(std_X_test,std_y_test)*100,2))\n",
    "    print('='*80)\n",
    "    print(f\"{c} impurity max tree depth of {depth} accuracy: {accuracies[j]}%\")\n",
    "    print('='*80)\n",
    "    cm = metrics.confusion_matrix(std_y_test,tree_models[j].predict(std_X_test))\n",
    "    print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "    print('='*80)\n",
    "    plt.rcParams.update({'font.size':15})\n",
    "    ax = plt.figure(figsize=(8,8))\n",
    "    sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion matrix\\nTesting accuracy score:{}%'.format(accuracies[j]),size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The highest accuracy with depth {depth}\")\n",
    "dot_data = export_graphviz(tree_models[0],filled=True,rounded=True,\n",
    "                           class_names=['phishing','non-phishing'],\n",
    "                           feature_names=[f'ch{i+1}' for i in range(std_X.shape[1])],out_file=None)\n",
    "dot_graph = graph_from_dot_data(dot_data)\n",
    "dot_graph.write_png('gini0.png')\n",
    "Image('gini0.png',height=1000,width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_models = []\n",
    "accuracies = []\n",
    "criterion = ['gini','entropy']\n",
    "depth = 6\n",
    "for j, c in enumerate(criterion):\n",
    "    tree_models.append(DecisionTreeClassifier(criterion=c,max_depth=depth,random_state=seed))\n",
    "    tree_models[j].fit(std_X,std_y)\n",
    "    accuracies.append(np.round(tree_models[j].score(std_X_test,std_y_test)*100,2))\n",
    "    print('='*80)\n",
    "    print(f\"{c} impurity max tree depth of {depth} accuracy: {accuracies[j]}%\")\n",
    "    print('='*80)\n",
    "    cm = metrics.confusion_matrix(std_y_test,tree_models[j].predict(std_X_test))\n",
    "    print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "    print('='*80)\n",
    "    plt.rcParams.update({'font.size':15})\n",
    "    ax = plt.figure(figsize=(8,8))\n",
    "    sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion matrix\\nTesting accuracy score:{}%'.format(accuracies[j]),size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The highest accuracy with depth {depth}\")\n",
    "dot_data = export_graphviz(tree_models[0],filled=True,rounded=True,\n",
    "                           class_names=['phishing','non-phishing'],\n",
    "                           feature_names=[f'ch{i+1}' for i in range(std_X.shape[1])],out_file=None)\n",
    "dot_graph = graph_from_dot_data(dot_data)\n",
    "dot_graph.write_png('gini1.png')\n",
    "Image('gini1.png',height=1000,width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adas = []\n",
    "for j in range(2):\n",
    "    adas.append(AdaBoostClassifier(base_estimator=tree_models[j],n_estimators=500,\n",
    "                                   learning_rate=0.1,random_state=seed))\n",
    "    adas[j].fit(std_X,std_y)\n",
    "    test_score = np.round(adas[j].score(std_X_test,std_y_test)*100,2)\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Adaboost with decision tree base {criterion[j]} of depth {depth} accuracy:\"\n",
    "          f\"{test_score}%\")\n",
    "    print(\"=\"*80)\n",
    "    cm = metrics.confusion_matrix(std_y_test,adas[j].predict(std_X_test))\n",
    "    print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "    print('='*80)\n",
    "    plt.rcParams.update({'font.size':15})\n",
    "    ax = plt.figure(figsize=(8,8))\n",
    "    sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion matrix\\nTesting accuracy score:{}%'.format(test_score),size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
