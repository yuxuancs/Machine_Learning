{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics,preprocessing,svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from pydotplus import graph_from_dot_data\n",
    "from IPython.display import Image\n",
    "from keras import layers, models\n",
    "from keras.optimizer_v2 import adam\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.utils import vis_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset citation:\n",
    "https://research.aalto.fi/en/datasets/phishstorm-phishing-legitimate-url-dataset\n",
    "\"\n",
    "Description\n",
    "URLs dataset with features built and used for evaluation in the paper \"PhishStorm: Detecting Phishing with Streaming Analytics\" published in IEEE TNSM.\n",
    "The dataset contains 96,018 URLs: 48,009 legitimate URLs and 48,009 phishing URLs.\n",
    "\n",
    "This is a CSV file where the \"domain\" column provides a unique identifier for each entry (which is actually a URL). The \"label\" column provides the domain entry status, 0: legitimate / 1:phishing.\n",
    "Other columns provide computed values for features introduced in [1].\n",
    "\n",
    "Please refer to the following publication when citing this dataset:\n",
    "[1] S. Marchal, J. Francois, R. State, and T. Engel. PhishStorm: Detecting Phishing with Streaming Analytics. IEEE Transactions on Network and Service Management (TNSM), 11(4):458-471, 2014.\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 69 # fix random seed\n",
    "df = pd.read_csv('urlset.csv',\n",
    "                 encoding='UTF-8',\n",
    "                 encoding_errors='ignore',\n",
    "                 on_bad_lines='skip',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>ranking</th>\n",
       "      <th>mld_res</th>\n",
       "      <th>mld.ps_res</th>\n",
       "      <th>card_rem</th>\n",
       "      <th>ratio_Rrem</th>\n",
       "      <th>ratio_Arem</th>\n",
       "      <th>jaccard_RR</th>\n",
       "      <th>jaccard_RA</th>\n",
       "      <th>jaccard_AR</th>\n",
       "      <th>jaccard_AA</th>\n",
       "      <th>jaccard_ARrd</th>\n",
       "      <th>jaccard_ARrem</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobell.it/70ffb52d079109dca5664cce6f317373782/...</td>\n",
       "      <td>10000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>107.611111</td>\n",
       "      <td>107.277778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.795729</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...</td>\n",
       "      <td>10000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>150.636364</td>\n",
       "      <td>152.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.768577</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>serviciosbys.com/paypal.cgi.bin.get-into.herf....</td>\n",
       "      <td>10000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>72.642857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.726582</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mail.printakid.com/www.online.americanexpress....</td>\n",
       "      <td>10000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>562.000000</td>\n",
       "      <td>590.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85964</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thewhiskeydregs.com/wp-content/themes/widescre...</td>\n",
       "      <td>10000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>24.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.748971</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              domain   ranking mld_res  \\\n",
       "0  nobell.it/70ffb52d079109dca5664cce6f317373782/...  10000000     1.0   \n",
       "1  www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...  10000000     0.0   \n",
       "2  serviciosbys.com/paypal.cgi.bin.get-into.herf....  10000000     0.0   \n",
       "3  mail.printakid.com/www.online.americanexpress....  10000000     0.0   \n",
       "4  thewhiskeydregs.com/wp-content/themes/widescre...  10000000     0.0   \n",
       "\n",
       "  mld.ps_res  card_rem  ratio_Rrem  ratio_Arem  jaccard_RR  jaccard_RA  \\\n",
       "0        0.0      18.0  107.611111  107.277778         0.0         0.0   \n",
       "1        0.0      11.0  150.636364  152.272727         0.0         0.0   \n",
       "2        0.0      14.0   73.500000   72.642857         0.0         0.0   \n",
       "3        0.0       6.0  562.000000  590.666667         0.0         0.0   \n",
       "4        0.0       8.0   29.000000   24.125000         0.0         0.0   \n",
       "\n",
       "   jaccard_AR  jaccard_AA jaccard_ARrd jaccard_ARrem  label  \n",
       "0         0.0         0.0          0.8      0.795729    1.0  \n",
       "1         0.0         0.0            0      0.768577    1.0  \n",
       "2         0.0         0.0            0      0.726582    1.0  \n",
       "3         0.0         0.0            0       0.85964    1.0  \n",
       "4         0.0         0.0            0      0.748971    1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96003, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### there are some unreadable row, we have to skip therefore the dataset contains total 96003 samples. However, there is two duplicates values, we have to drop them. Therefore, the dataset contains total 96001 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove other unneccessary columns, only keep url and label and make sure url type is string\n",
    "df = df[['domain','label']].astype({'domain':str,'label':float})\n",
    "df = df.drop_duplicates(ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobell.it/70ffb52d079109dca5664cce6f317373782/...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>serviciosbys.com/paypal.cgi.bin.get-into.herf....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mail.printakid.com/www.online.americanexpress....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thewhiskeydregs.com/wp-content/themes/widescre...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              domain  label\n",
       "0  nobell.it/70ffb52d079109dca5664cce6f317373782/...    1.0\n",
       "1  www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...    1.0\n",
       "2  serviciosbys.com/paypal.cgi.bin.get-into.herf....    1.0\n",
       "3  mail.printakid.com/www.online.americanexpress....    1.0\n",
       "4  thewhiskeydregs.com/wp-content/themes/widescre...    1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data total 402 unique characters:\n",
      "['\\x01' '\\x02' '\\x03' '\\x04' '\\x05' '\\x06' '\\x07' '\\x08' '\\t' '\\n' '\\x0b'\n",
      " '\\x0c' '\\r' '\\x0e' '\\x0f' '\\x10' '\\x11' '\\x12' '\\x13' '\\x14' '\\x15'\n",
      " '\\x16' '\\x17' '\\x18' '\\x19' '\\x1a' '\\x1b' '\\x1c' '\\x1d' '\\x1e' '\\x1f' ' '\n",
      " '!' '\"' '#' '$' '%' '&' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0' '1' '2'\n",
      " '3' '4' '5' '6' '7' '8' '9' ':' ';' '<' '=' '>' '?' '@' 'A' 'B' 'C' 'D'\n",
      " 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V'\n",
      " 'W' 'X' 'Y' 'Z' '[' '\\\\' ']' '^' '_' '`' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h'\n",
      " 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z'\n",
      " '{' '|' '}' '~' '\\x7f' '\\x87' '\\x8d' '\\x8f' '\\x92' '¥' '\\xad' '¸' 'Æ' 'Ç'\n",
      " 'Î' 'Ò' 'Ó' '×' 'Û' 'à' 'ñ' 'ā' 'ą' 'Č' 'č' 'ģ' 'ī' 'į' 'Ĳ' 'Ŀ' 'ń' 'ň'\n",
      " 'Ŧ' 'ũ' 'Ū' 'ů' 'Ż' 'ſ' 'ƃ' 'Ƈ' 'Ɖ' 'Ɗ' 'ƭ' 'ư' 'Ʊ' 'ǆ' 'Ǔ' 'Ǡ' 'ǡ' 'Ǥ'\n",
      " 'Ǭ' 'ǽ' 'ȅ' 'ȍ' 'Ȕ' 'Ș' 'Ȳ' 'Ⱦ' 'Ɍ' 'ɒ' 'ɔ' 'ɦ' 'ɧ' 'ɮ' 'ʃ' 'ʋ' 'ʒ' 'ʖ'\n",
      " 'ʟ' 'ʡ' 'ʩ' 'ʭ' 'ʴ' 'ʸ' 'ˊ' 'ˎ' '˗' '˛' '˞' '˟' 'ˠ' '˨' '˱' '́' '̅' '̈'\n",
      " '̉' '̍' '̜' '̣' '̤' '̫' '̯' '̶' '͑' '͙' 'ͦ' 'ͧ' 'ͳ' '͵' 'Ξ' 'Ρ' 'Χ' 'Ω'\n",
      " 'ΰ' 'θ' 'μ' 'ς' 'ώ' 'ϖ' 'ϗ' 'ϝ' 'ϟ' 'Ϩ' 'Ϭ' 'ϸ' 'Ё' 'Љ' 'А' 'Ы' 'к' 'щ'\n",
      " 'ё' 'љ' 'Ѣ' 'Ѧ' 'Ѫ' 'Ѯ' 'ѳ' 'ѿ' '҃' 'Ҏ' 'Ғ' 'Ҟ' 'ҥ' 'ҩ' 'Ұ' 'Ҹ' 'Ҽ' 'Ӌ'\n",
      " 'ӎ' 'ӗ' 'ӛ' 'Ӟ' 'ӡ' 'ӵ' 'ӹ' 'ӽ' 'ԅ' 'Ԓ' 'ԣ' 'ԫ' 'ԯ' 'Ծ' 'Ռ' 'Վ' '՝' 'ձ'\n",
      " 'ջ' 'փ' 'և' '\\u058b' '֒' '֖' '֢' '֥' 'ָ' 'ׂ' '׆' '\\u05ce' 'כ' 'פ' 'ר'\n",
      " '\\u05f9' '\\u0601' '؈' '؉' 'ؓ' 'ؔ' 'ؘ' 'ؚ' '\\u061c' 'ؤ' 'ث' 'ظ' 'ل' 'ّ'\n",
      " 'ٙ' 'ٛ' 'ٝ' '٦' 'ٮ' 'ٳ' 'ٷ' 'ٻ' 'پ' 'ڃ' 'ڒ' 'ښ' 'ڜ' 'ڡ' 'ڴ' 'ڸ' 'ۂ' 'ۓ'\n",
      " 'ۗ' 'ۛ' '۠' 'ۤ' '۬' '۱' '۵' '۶' '۸' '۹' 'ۼ' '܁' '܉' '܍' 'ܔ' 'ܘ' 'ܚ' 'ܧ'\n",
      " 'ܺ' '݄' '݈' '\\u074c' 'ݑ' 'ݙ' 'ݝ' 'ݪ' 'ݮ' 'ݻ' 'ހ' 'އ' 'ޖ' 'ޟ' 'ޥ' 'ް'\n",
      " '\\u07b8' '߂' 'ߓ' 'ߗ' 'ߛ' 'ߟ' 'ߨ' '߬' '߯' '߱' 'ෘ' '∙' '⊿' 'Ȿ' '㋣' '㸡' '剩'\n",
      " '嫪' '庫' '彍' '湎' '虠' '讣' '霽' '鯷' '鯿' '龾' 'ꎙ' '\\uab1a' '꭛' '굔' '꽛' '랄' '젷'\n",
      " '쭠' '퍏' '\\ueb29' '\\uf1a6' '\\U0006d7b7' '\\U000d6bab' '\\U00105966']\n"
     ]
    }
   ],
   "source": [
    "all_char = list(''.join(df.domain.values.tolist()))\n",
    "unique_char = np.unique(all_char,return_counts=True)\n",
    "print(\"Raw data total {} unique characters:\".format(unique_char[0].shape[0]))\n",
    "print(unique_char[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### preprocessing data\n",
    "1. fit character into numerical values\n",
    "2. only keep the url length up to 75 characters\n",
    "3. if url does not have 75 characters then padding in front with 0\n",
    "4. remove all $\\pm\\infty$ and nan values\n",
    "5. balance binary dataset\n",
    "-------------------------------------\n",
    "6. use PCA to reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2175 possible length of characters\n",
      "================================================================================\n",
      "Total 96001 samples and only use 75 features\n",
      "================================================================================\n",
      "After balance sampling:\n",
      "|phishing:  47902  | non-phishing: 47902\n",
      "================================================================================\n",
      "split dataset into training and testing:\n",
      "Training size: 76643; Testing size: 19161\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "char_tok = text.Tokenizer(char_level=True) # character function\n",
    "char_tok.fit_on_texts(df.domain) # fit character\n",
    "data = char_tok.texts_to_sequences(df.domain)\n",
    "num_char = len(char_tok.word_index)+1\n",
    "max_length = max([len(n) for n in data])\n",
    "length = 75\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "print(\"Total {} possible length of characters\".format(max_length)) # check maximum possible length\n",
    "print('='*80)\n",
    "data = sequence.pad_sequences(data,maxlen=length,dtype=np.float64) \n",
    "print(\"Total {} samples and only use {} features\".format(*data.shape))\n",
    "print('='*80)\n",
    "# save data and its target to a new dataframe\n",
    "data = pd.DataFrame(np.hstack((data,df[['label']].values)))\n",
    "# drop nan, inf, or -inf values\n",
    "data = data.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "# use PCA to reduce dimension of feature\n",
    "columns = data.columns\n",
    "rus = RandomUnderSampler(random_state=seed) # random balance smaple function\n",
    "standard = preprocessing.StandardScaler() # standardize data function\n",
    "X, y = rus.fit_resample(data[columns[:-1]],data[columns[-1]])\n",
    "print(\"After balance sampling:\")\n",
    "print(f\"|phishing:  {len(y[y==1])}  | non-phishing: {len(y[y==0])}\")\n",
    "print('='*80)\n",
    "# split dataset for training, validation and testing\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=seed,stratify=y)\n",
    "print(\"split dataset into training and testing:\")\n",
    "print(f\"Training size: {y_train.shape[0]}; Testing size: {y_test.shape[0]}\")\n",
    "print('='*80)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(num_char=373,length=75):\n",
    "    '''\n",
    "    convolutional neural network\n",
    "    '''\n",
    "    model = models.Sequential([Embedding(num_char,128,input_length=length), # embedding\n",
    "                               layers.Dropout(0.5), \n",
    "                               layers.Conv1D(256, 3), # conv layer 1\n",
    "                               layers.ELU(), # Exponential Linear Unit\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Conv1D(512, 6), # conv layer 2\n",
    "                               layers.ELU(), # Exponential Linear Unit\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Conv1D(1024,8), # conv layer 3\n",
    "                               layers.ELU(), # Exponential Linear Unit\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Flatten(),\n",
    "                               layers.Dense(1024,activation='relu'), # hidden layer 1\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Dense(256, activation='relu'), # hidden layer 2\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Dense(64,  activation='relu'), # hidden layer 3\n",
    "                               layers.BatchNormalization(), # batach normalization \n",
    "                               layers.Dropout(0.5),\n",
    "                               layers.Dense(1,activation='sigmoid')]) # output layer\n",
    "    model_compile(model)\n",
    "    return model\n",
    "\n",
    "def LSTM_model(num_char=373,length=75):\n",
    "    '''\n",
    "    model with one embedding layer,\n",
    "    follow up with 25% drop out and batach normalization\n",
    "    lstm layer with 128 units and 50% recurrent drop out\n",
    "    '''\n",
    "    model =models.Sequential([Embedding(num_char,128,input_length=length), # embedding\n",
    "                              layers.Dropout(0.25),\n",
    "                              layers.BatchNormalization(), # batach normalization \n",
    "                              layers.LSTM(128,recurrent_dropout=0.5), # long-short term memory\n",
    "                              layers.Dropout(0.5),\n",
    "                              layers.Flatten(),\n",
    "                              layers.Dense(1,activation='sigmoid')]) # output layer\n",
    "    model_compile(model)\n",
    "    return model\n",
    "\n",
    "def LSTM_larger_model(num_char=373,length=75):\n",
    "    '''\n",
    "    model with one embedding layer,\n",
    "    follow up with 25% drop out and batach normalization\n",
    "    lstm layer with 256 units and 50% recurrent drop out\n",
    "    lstm layer with 512 units and 50% recurrent drop out\n",
    "    '''\n",
    "    model =models.Sequential([Embedding(num_char,128,input_length=length), # embedding\n",
    "                              layers.Dropout(0.25),\n",
    "                              layers.BatchNormalization(), # batach normalization \n",
    "                              layers.LSTM(256,recurrent_dropout=0.5,\n",
    "                                          return_sequences=True,\n",
    "                                          input_shape=(75,128)), # long-short term memory\n",
    "                              layers.Dropout(0.5),\n",
    "                              layers.BatchNormalization(), # batach normalization\n",
    "                              layers.LSTM(512,recurrent_dropout=0.5), # long-short term memory\n",
    "                              layers.Dropout(0.5),\n",
    "                              layers.Flatten(),\n",
    "                              layers.Dense(1,activation='sigmoid')]) # output layer\n",
    "    model_compile(model)\n",
    "    return model\n",
    "    \n",
    "\n",
    "def model_compile(model):\n",
    "    '''\n",
    "    model compile with binary corss entropy, adam optimizer with learning rate 0.005\n",
    "    '''\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam.Adam(learning_rate=0.005),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "def train_model(model,X_train,y_train,epochs=10,batch_size=32,filename='cnn'):\n",
    "#     print(\"model summary\")\n",
    "#     print(model.summary())\n",
    "    print(f\"training model with {epochs} epochs and batches of size {batch_size}:\")\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint(filename+'.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7,\n",
    "                                       verbose=1,min_delta=1e-4, mode='min')\n",
    "    return model.fit(X_train,y_train,validation_split=0.2,epochs=epochs,\n",
    "                     batch_size=batch_size,use_multiprocessing=True,\n",
    "                     callbacks=[earlyStopping, mcp_save,reduce_lr_loss])\n",
    "\n",
    "def load_best_model(model,filename='cnn'):\n",
    "    return model.load_weights(filepath = filename+'.hdf5')\n",
    "\n",
    "\n",
    "def evaluate_model(model,X_test,y_test):\n",
    "    print(\"model summary\")\n",
    "    print(model.summary())\n",
    "    print(f\"testing model:\")\n",
    "    result = model.evaluate(X_test,y_test)\n",
    "    print(\"Model score: {:.04f} and accuracy: {:0.2f}%\".format(result[0],result[1]*100))\n",
    "    \n",
    "def classification_report(model,X_test,y_test):\n",
    "    print(\"claissification report:\")\n",
    "    print(metrics.classification_report(y_test,\n",
    "                                  np.where(model.predict(X_test,\n",
    "                                                         use_multiprocessing=True)>0.5,\n",
    "                                           1,0)))\n",
    "    \n",
    "def export_plot(model,fname='cnn'):\n",
    "    '''\n",
    "    plot modell's structure\n",
    "    '''\n",
    "    vis_utils.plot_model(model,to_file=fname+'.png',show_shapes=True,\n",
    "                         show_layer_names=True,expand_nested=True,dpi=96,)\n",
    "    \n",
    "def plot_acc(ax,hist,title):\n",
    "    val = title if title == 'Accuracy' else 'Loss'\n",
    "    ax.plot(hist.history[val.lower()],lw=2)\n",
    "    ax.plot(hist.history['val_'+val.lower()],lw=2)\n",
    "    ax.set(title=title,ylabel=val,xlabel='Epoch')\n",
    "    ax.legend(['Train','Test'],fontsize='xx-large')\n",
    "    ax.grid(True)\n",
    "\n",
    "def false_negative_rate(cm):\n",
    "    '''\n",
    "    compute false negative rate\n",
    "    '''\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return fn/(fn+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 17:47:38.134514: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CNN_model(num_char,length)\n",
    "export_plot(cnn_model,'cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model with 10 epochs and batches of size 32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 17:47:39.476563: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 806/1917 [===========>..................] - ETA: 8:35 - loss: 0.4899 - accuracy: 0.7785"
     ]
    }
   ],
   "source": [
    "cnn_hist = train_model(cnn_model,X_train,y_train,filename='cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_best_model(cnn_model,filename='cnn')\n",
    "plt.rcParams.update({'font.size':15})\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,8))\n",
    "ax = ax.flatten()\n",
    "plot_acc(ax[0],cnn_hist,'Accuracy')\n",
    "plot_acc(ax[1],cnn_hist,'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(cnn_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "y_pred = np.where(cnn_model.predict(X_test,use_multiprocessing=True)>0.5,1,0)\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:'\n",
    "          '{:0.2f}%'.format(metrics.accuracy_score(y_test,y_pred)*100),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(cnn_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_model(num_char,length)\n",
    "export_plot(lstm_model,'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lstm_hist = train_model(lstm_model,X_train,y_train,filename='lstm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_best_model(lstm_model,filename='lstm1')\n",
    "plt.rcParams.update({'font.size':15})\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,8))\n",
    "ax = ax.flatten()\n",
    "plot_acc(ax[0],lstm_hist,'Accuracy')\n",
    "plot_acc(ax[1],lstm_hist,'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lstm_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "y_pred = np.where(lstm_model.predict(X_test,use_multiprocessing=True)>0.5,1,0)\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:'\n",
    "          '{:0.2f}%'.format(metrics.accuracy_score(y_test,y_pred)*100),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(lstm_model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_lstm = LSTM_larger_model(num_char,length)\n",
    "export_plot(larger_lstm,'larger_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_lstm_hist = train_model(larger_lstm,X_train,y_train,filename='lstm2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_best_model(larger_lstm_model,filename='lstm2')\n",
    "plt.rcParams.update({'font.size':15})\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,8))\n",
    "ax = ax.flatten()\n",
    "plot_acc(ax[0],larger_lstm_hist,'Accuracy')\n",
    "plot_acc(ax[1],larger_lstm_hist,'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(larger_lstm,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "y_pred = np.where(larger_lstm.predict(X_test,use_multiprocessing=True)>0.5,1,0)\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:'\n",
    "          '{:0.2f}%'.format(metrics.accuracy_score(y_test,y_pred)*100),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(larger_lstm,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression without feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression without feature reduction:\")\n",
    "logit = LogisticRegression(penalty='l2',tol=0.0001,C=1,random_state=seed,\n",
    "                           solver='liblinear',multi_class='ovr',max_iter=100,verbose=0)\n",
    "logit.fit(X_train,y_train)\n",
    "test_score = np.round(logit.score(X_test,y_test)*100,2)\n",
    "print('='*80)\n",
    "print(f\"|+| test accuracy: \\t{test_score}%\")\n",
    "print('='*80)\n",
    "cm = metrics.confusion_matrix(y_test,logit.predict(X_test))\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:'\n",
    "          '{:0.2f}%'.format(metrics.accuracy_score(y_test,y_pred)*100),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modified from: Selecting dimensionality reduction with Pipeline and GridSearchCV\n",
    "orginal authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre\n",
    "'''\n",
    "pipe = Pipeline([# the reduce_dim stage is populated by the param_grid\n",
    "                 (\"reduce_dim\", \"passthrough\"),\n",
    "                 (\"classify\", svm.LinearSVC(dual=False, max_iter=10000)),\n",
    "                ])\n",
    "\n",
    "N_FEATURES_OPTIONS = [15,30,45,60] # reduction features\n",
    "C_OPTIONS = [1,10]\n",
    "param_grid = [{\"reduce_dim\": [PCA(iterated_power=7,random_state=seed)],\n",
    "               \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n",
    "               \"classify__C\": C_OPTIONS,}]\n",
    "reducer_labels = [\"PCA\"]\n",
    "\n",
    "grid = GridSearchCV(pipe,n_jobs=1,param_grid=param_grid)\n",
    "grid.fit(X, y)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_[\"mean_test_score\"])\n",
    "# scores are in the order of param_grid iteration, which is alphabetical\n",
    "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "# select score for best C\n",
    "mean_scores = mean_scores.max(axis=0)\n",
    "bar_offsets = np.arange(len(N_FEATURES_OPTIONS)) * (len(reducer_labels) + 1) + 0.5\n",
    "\n",
    "plt.figure()\n",
    "COLORS = \"bgrcmyk\"\n",
    "for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n",
    "    plt.bar(bar_offsets+i,reducer_scores,label=label,color=COLORS[i])\n",
    "\n",
    "plt.title(\"Comparing feature reduction techniques\")\n",
    "plt.xlabel(\"Reduced number of features\")\n",
    "plt.xticks(bar_offsets, N_FEATURES_OPTIONS)\n",
    "plt.ylabel(\"classification accuracy\")\n",
    "plt.ylim((0, 1))\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "# only use 30 features from pca\n",
    "pca = PCA(n_components=15,iterated_power=7,random_state=seed)\n",
    "new_X = pca.fit_transform(X)\n",
    "\n",
    "# standardize data\n",
    "new_X = standard.fit_transform(new_X) # normalize all features\n",
    "std_X = new_X[X_train.index.values,:] # allocate X train, sample \n",
    "std_y = y.values[y_train.index.values] # allocate train target\n",
    "std_X_test = new_X[X_test.index.values,:] # allocate X test \n",
    "std_y_test = y.values[y_test.index.values] # allocate test target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(penalty='l2',tol=0.0001,C=1,random_state=seed,\n",
    "                           solver='liblinear',multi_class='ovr',max_iter=100,verbose=1)\n",
    "logit.fit(std_X,std_y)\n",
    "test_score = np.round(logit.score(std_X_test,std_y_test)*100,2)\n",
    "print('='*80)\n",
    "print(f\"|+| test accuracy: \\t{test_score}%\")\n",
    "print('='*80)\n",
    "cm = metrics.confusion_matrix(std_y_test,logit.predict(std_X_test))\n",
    "print(\"false negative rate: {:0.4f}%\".format(false_negative_rate(cm)*100))\n",
    "print('='*80)\n",
    "plt.rcParams.update({'font.size':15})\n",
    "ax = plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm,annot=True,fmt=\".0f\",linewidths=.5,square=True,cmap='rainbow_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix\\nTesting accuracy score:{}%'.format(test_score),size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = metrics.precision_recall_curve(std_y_test,logit.predict(std_X_test))\n",
    "disp = metrics.PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "disp.plot()\n",
    "disp.ax_.set(title=\"Precision-Recall curve\")\n",
    "disp.ax_.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1 # SVM regularization parameter\n",
    "max_iter = 100000\n",
    "svm_models = [svm.LinearSVC(C=C,max_iter=max_iter),\n",
    "              svm.SVC(kernel='rbf',gamma='auto',max_iter=max_iter,C=C)]\n",
    "svm_models = [clf.fit(std_X,std_y) for clf in svm_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_titles = ['Linear SVC', 'SVC with RBF kernel']\n",
    "for i, clf in enumerate(svm_models):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"||{svm_titles[i]}\\ttesting score: {np.round(clf.score(std_X_test,std_y_test)*100,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_models = []\n",
    "accuracies = []\n",
    "criterion = ['gini','entropy']\n",
    "depth = 3\n",
    "for j, c in enumerate(criterion):\n",
    "    tree_models.append(DecisionTreeClassifier(criterion=c,max_depth=depth,random_state=seed))\n",
    "    tree_models[j].fit(std_X,std_y)\n",
    "    accuracies.append(np.round(tree_models[j].score(std_X_test,std_y_test)*100,2))\n",
    "    print('='*80)\n",
    "    print(f\"{c} impurity max tree depth of {depth} accuracy: {accuracies[j]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The highest accuracy with depth {depth}\")\n",
    "dot_data = export_graphviz(tree_models[0],filled=True,rounded=True,\n",
    "                           class_names=['phishing','non-phishing'],\n",
    "                           feature_names=np.arange(std_X.shape[1])+1,out_file=None)\n",
    "dot_graph = graph_from_dot_data(dot_data)\n",
    "dot_graph.write_png('gini0.png')\n",
    "Image('gini0.png',height=1000,width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_models = []\n",
    "accuracies = []\n",
    "criterion = ['gini','entropy']\n",
    "depth = 6\n",
    "for j, c in enumerate(criterion):\n",
    "    tree_models.append(DecisionTreeClassifier(criterion=c,max_depth=depth,random_state=seed))\n",
    "    tree_models[j].fit(std_X,std_y)\n",
    "    accuracies.append(np.round(tree_models[j].score(std_X_test,std_y_test)*100,2))\n",
    "    print('='*80)\n",
    "    print(f\"{c} impurity max tree depth of {depth} accuracy: {accuracies[j]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The highest accuracy with depth {depth}\")\n",
    "dot_data = export_graphviz(tree_models[0],filled=True,rounded=True,\n",
    "                           class_names=['phishing','non-phishing'],\n",
    "                           feature_names=np.arange(std_X.shape[1])+1,out_file=None)\n",
    "dot_graph = graph_from_dot_data(dot_data)\n",
    "dot_graph.write_png('gini1.png')\n",
    "Image('gini1.png',height=1000,width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adas = []\n",
    "for j in range(2):\n",
    "    adas.append(AdaBoostClassifier(base_estimator=tree_models[j],n_estimators=500,\n",
    "                                   learning_rate=0.1,random_state=seed))\n",
    "    adas[j].fit(std_X,std_y)\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Adaboost with tree base depth {depth} accuracy:\"\n",
    "          f\"{np.round(adas[j].score(std_X_test,std_y_test)*100,2)}%\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
